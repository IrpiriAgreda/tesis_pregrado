{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import _pickle as cPickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 5981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, fc3_dims,\n",
    "                 n_actions):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.fc3_dims = fc3_dims\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "            \n",
    "        if type(self.fc2_dims) != bool:\n",
    "            self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "                \n",
    "            if type(self.fc3_dims) != bool:\n",
    "                self.fc3 = nn.Linear(self.fc2_dims, self.fc3_dims)\n",
    "                    \n",
    "                self.pi = nn.Linear(self.fc3_dims, n_actions)\n",
    "                self.v = nn.Linear(self.fc3_dims, n_actions)\n",
    "            else:\n",
    "                self.pi = nn.Linear(self.fc2_dims, n_actions)\n",
    "                self.v = nn.Linear(self.fc2_dims, n_actions)\n",
    "        else:\n",
    "            self.pi = nn.Linear(self.fc1_dims, n_actions)\n",
    "            self.v = nn.Linear(self.fc1_dims, n_actions)\n",
    "            \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        x = F.relu(self.fc1(observation))\n",
    "        \n",
    "        if type(self.fc2_dims) != bool:\n",
    "            x = F.relu(self.fc2(x))\n",
    "                \n",
    "            if type(self.fc3_dims) != bool:\n",
    "                x = F.relu(self.fc3(x))\n",
    "        \n",
    "        pi = self.pi(x)\n",
    "        v = self.v(x)\n",
    "        return (pi, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\" Agent class for use with a single actor critic network that shares\n",
    "        the lowest layers. For use with more complex environments such as\n",
    "        the discrete lunar lander\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, input_dims, gamma=0.001,\n",
    "                 layer1_size=32, layer2_size=64,layer3_size=128, n_actions=n_actions):\n",
    "        self.gamma = gamma\n",
    "        self.actor_critic = ActorCriticNetwork(alpha, input_dims, layer1_size,\n",
    "                                    layer2_size, layer3_size, n_actions)\n",
    "\n",
    "        self.log_probs = None\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        probabilities, _ = self.actor_critic.forward(observation)\n",
    "        probabilities = F.softmax(probabilities)\n",
    "        action_probs = T.distributions.Categorical(probabilities)\n",
    "        action = action_probs.sample()\n",
    "        #log_probs = action_probs.log_prob(action)\n",
    "        log_probs = action_probs.log_prob(T.tensor(range(5981), device=device))\n",
    "        self.log_probs = log_probs\n",
    "\n",
    "        return action_probs.probs\n",
    "\n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        self.actor_critic.optimizer.zero_grad()\n",
    "\n",
    "        _, critic_value_ = self.actor_critic.forward(new_state)\n",
    "        _, critic_value = self.actor_critic.forward(state)\n",
    "\n",
    "        delta = reward + self.gamma*critic_value_*(1-int(done)) - critic_value\n",
    "\n",
    "        actor_loss = -self.log_probs * delta\n",
    "        critic_loss = delta**2\n",
    "        \n",
    "        (actor_loss + critic_loss).sum().backward()\n",
    "\n",
    "        self.actor_critic.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "    import traci\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumoCmd = [\"/usr/bin/sumo/bin/sumo\", \"-c\", \"../sumo_simulation/sim_config/km2_centro/scenario/osm.sumocfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(num_episodes, date, exp_name, simconfig, layers, agent):\n",
    "    #Main parameters\n",
    "    current_ep = 0\n",
    "    \n",
    "    if not os.path.exists('./output_weights/policy/{}'.format(exp_name)):\n",
    "        os.mkdir('./output_weights/policy/{}'.format(exp_name))\n",
    "    if not os.path.exists('./output_weights/target/{}'.format(exp_name)):\n",
    "        os.mkdir('./output_weights/target/{}'.format(exp_name))\n",
    "\n",
    "    if simconfig == 0:\n",
    "        sumoCmd = [\"/usr/bin/sumo/bin/sumo\", \"-c\", \"../sumo_simulation/sim_config/km2_centro/scenario/osm.sumocfg\"]\n",
    "    else:\n",
    "        sumoCmd = [\"/usr/bin/sumo/bin/sumo\", \"-c\", \"../sumo_simulation/sim_config/km2_centro/scenario_{}/osm.sumocfg\".format(simconfig)]\n",
    "\n",
    "    action_dict = cPickle.load(open('../sumo_simulation/input/action_to_zone_km2_centro.pkl', 'rb'))\n",
    "\n",
    "\n",
    "    with open('output/rewards_gamma001_experiment_sc{}_layers_{}_{}'.format(simconfig, layers, date), 'a') as reward_file:\n",
    "        for i_episode in range(num_episodes):\n",
    "            #print(\"Episode {}\".format(i_episode))\n",
    "\n",
    "            state = np.zeros(6)\n",
    "            reward = 0\n",
    "            done = 0\n",
    "\n",
    "            #Start simulation\n",
    "            traci.start(sumoCmd)\n",
    "            id_list = traci.edge.getIDList()\n",
    "            lane_id_list = traci.lane.getIDList()\n",
    "\n",
    "            #Run simulation steps\n",
    "            traci_ep = 0\n",
    "            for w in range(86400):\n",
    "\n",
    "                if traci_ep % 3600 == 0 and traci_ep != 0:\n",
    "                    state = T.tensor(state, device=device, dtype=T.float)\n",
    "                    \n",
    "                    #Start agent interaction\n",
    "                    action = agent.choose_action(state)\n",
    "\n",
    "                    #Apply regulation and run steps\n",
    "                    reg_action = action > 0\n",
    "                    #lane_indices = (reg_action == 1).nonzero().view(-1)\n",
    "\n",
    "                    for index, lane_id in enumerate(reg_action.view(-1)):\n",
    "                    #for lane_id in lane_indices:\n",
    "                        if lane_id.item() == 1:\n",
    "                            if action_dict[index] is not None:\n",
    "                                traci.lane.setDisallowed(action_dict[index], ['truck'])\n",
    "                            else:\n",
    "                                reward -= 10000\n",
    "                        else:\n",
    "                            if action_dict[index] is not None:\n",
    "                                traci.lane.setAllowed(action_dict[index], ['truck'])\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "                    #Get simulation values\n",
    "                    co2 = [traci.lane.getCO2Emission(edge_id) for edge_id in lane_id_list]\n",
    "                    co = [traci.lane.getCOEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    nox = [traci.lane.getNOxEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    pmx = [traci.lane.getPMxEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    noise = [traci.lane.getNoiseEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    fuel = [traci.lane.getFuelConsumption(edge_id) for edge_id in lane_id_list]\n",
    "\n",
    "                    sim_results = np.array([co2, co, pmx, nox, noise, fuel])\n",
    "\n",
    "                    next_state = np.transpose(sim_results).mean(axis=0)\n",
    "\n",
    "                    vehicle_id_list = traci.vehicle.getIDList()\n",
    "                    vehicle_types = [traci.vehicle.getTypeID(v_id) for v_id in vehicle_id_list]\n",
    "                    vehicle_co2 = [traci.vehicle.getCO2Emission(v_id) for i, v_id in enumerate(vehicle_id_list)\n",
    "                                  if 'truck' in vehicle_types[i]]\n",
    "\n",
    "                    try:\n",
    "                        reward += (sum(vehicle_co2)/len(vehicle_co2)) * -1\n",
    "                    except:\n",
    "                        reward += 0\n",
    "\n",
    "                    #Convert to torch tensors\n",
    "                    reward = T.tensor([reward], device=device, dtype=T.float)\n",
    "                    state = T.tensor(state, device=device, dtype=T.float)\n",
    "                    next_state = T.tensor(next_state, device=device, dtype=T.float)\n",
    "\n",
    "                    #print('Reward at timestep {t}: {r}'.format(t=traci_ep/3600,r=reward.item()))\n",
    "                    reward_file.write(','.join([str(i_episode), str(traci_ep/3600), str(reward.item())])+'\\n')\n",
    "\n",
    "\n",
    "                    action = T.tensor(action, device=device, dtype=T.long).view(-1)\n",
    "                    \n",
    "                    #Optimize agent\n",
    "                    agent.learn(state, reward, next_state, done)\n",
    "                    \n",
    "                    state += next_state\n",
    "\n",
    "                traci.simulationStep()\n",
    "                traci_ep += 1\n",
    "\n",
    "            traci.close(False)\n",
    "\n",
    "            T.save(agent.actor_critic.state_dict(), './output_weights/policy/{}/ac_weights_experiment_ep_{}.pt'.format(exp_name, i_episode))\n",
    "            T.save(agent.actor_critic.optimizer.state_dict(), './output_weights/policy/{}/ac_optimizer_experiment_ep_{}.pt'.format(exp_name, i_episode))\n",
    "        print('Complete')\n",
    "        T.save(agent.actor_critic.state_dict(), './output_weights/policy/{}/ac_weights_experiment_ep_{}.pt'.format(exp_name, i_episode))\n",
    "        T.save(agent.actor_critic.optimizer.state_dict(), './output_weights/policy/{}/ac_optimizer_experiment_ep_{}.pt'.format(exp_name, i_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d79b48f34c54696a8a0c21b94467ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a79a12985c478aab40cecbf232a4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e29ed96f942a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   n_actions=n_actions)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'km2_centro_pg_scenario_{}_layers_{}_date_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3755abca8eb3>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(num_episodes, date, exp_name, simconfig, layers, agent)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mtraci_ep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/sumo/tools/traci/__init__.py\u001b[0m in \u001b[0;36msimulationStep\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_stepListeners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# manage stepListeners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36msimulationStep\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!BBd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubscriptionResults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subscriptionMapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0msubscriptionResults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# print(\"python_sendExact: '%s'\" % ' '.join(map(lambda x : \"%X\" % ord(x), self._string)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recvExact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36m_recvExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "today = str(datetime.datetime.today())[:10]\n",
    "\n",
    "for exp in tqdm([0,2,3]):\n",
    "    for layers in tqdm([1,2,3]):\n",
    "        if layers == 1:\n",
    "            agent = Agent(alpha=0.001, input_dims=[6], gamma=0.001,\n",
    "                  n_actions=n_actions, layer2_size=False, layer3_size=False)\n",
    "        elif layers == 2:\n",
    "            agent = Agent(alpha=0.001, input_dims=[6], gamma=0.001,\n",
    "                  n_actions=n_actions, layer3_size=False)\n",
    "        else:\n",
    "            agent = Agent(alpha=0.001, input_dims=[6], gamma=0.001,\n",
    "                  n_actions=n_actions)\n",
    "        \n",
    "        run_experiments(1500, today, 'km2_centro_pg_scenario_{}_layers_{}_date_{}'.format(exp, layers, today), exp, layers, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "c912e28f322a44879a12c30a733600c4",
   "lastKernelId": "b6210f74-6a06-4f56-98e6-deec7880b394"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
