{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import imageio\n",
    "import os\n",
    "import sys\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "    import traci\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dict = {\n",
    "    'base': {\n",
    "        'weight_path': './output_weights/km2_centro_test_scenario_0_27082019/policy_net_weights_experiment_ep_2999.pt',\n",
    "        'config': \"../sumo_simulation/sim_config/km2_centro/scenario/osm.sumocfg\",\n",
    "        'plot_name': 'scenario_0'\n",
    "    },\n",
    "    'case_2x': {\n",
    "        'weight_path': './output_weights/policy/km2_centro_test_scenario_2_27082019/policy_net_weights_experiment_ep_2999.pt',\n",
    "        'config': \"../sumo_simulation/sim_config/km2_centro/scenario_2/osm.sumocfg\",\n",
    "        'plot_name': 'scenario_2'\n",
    "    },\n",
    "    'case_4x': {\n",
    "        'weight_path': './output_weights/policy/km2_centro_test_scenario_3_27082019/policy_net_weights_experiment_ep_2999.pt',\n",
    "        'config': \"../sumo_simulation/sim_config/km2_centro/scenario_3/osm.sumocfg\",\n",
    "        'plot_name': 'scenario_3'\n",
    "    },\n",
    "    'case_8x': {\n",
    "        'config': \"../sumo_simulation/sim_config/km2_centro/scenario_4/osm.sumocfg\",\n",
    "        'plot_name': 'scenario_4'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.mlp1 = nn.Linear(6,32)\n",
    "#         self.mlp2 = nn.Linear(32,64)\n",
    "#         self.mlp3 = nn.Linear(64,128)\n",
    "#         self.mlp4 = nn.Linear(128,256)\n",
    "#         self.mlp5 = nn.Linear(256,256)\n",
    "#         self.head = nn.Linear(256, outputs)\n",
    "        self.head = nn.Linear(32, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.mlp1(x))\n",
    "#         x = F.relu(self.mlp2(x))\n",
    "#         x = F.relu(self.mlp3(x))\n",
    "#         x = F.relu(self.mlp4(x))\n",
    "#         x = F.relu(self.mlp5(x))\n",
    "\n",
    "        return F.relu(self.head(x.view(x.size(0), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.001\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "n_actions = 5981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) *         math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state.view([-1,6]))\n",
    "    else:\n",
    "        return torch.randn(n_actions, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state).view(-1,6)\n",
    "    #action_batch = torch.cat(batch.action).view(-1,n_actions)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch)#.gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros((BATCH_SIZE, 5981), device=device)\n",
    "\n",
    "    nsv = target_net(non_final_next_states.view(-1,6)).detach()\n",
    "\n",
    "    next_state_values[nsv > 0] = nsv[nsv > 0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = ((next_state_values * GAMMA).t() + reward_batch).t()\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(num_episodes, date, exp_name, simconfig):\n",
    "    #Main parameters\n",
    "    current_ep = 0\n",
    "    \n",
    "    if not os.path.exists('./output_weights/policy/{}'.format(exp_name)):\n",
    "        os.mkdir('./output_weights/policy/{}'.format(exp_name))\n",
    "    if not os.path.exists('./output_weights/target/{}'.format(exp_name)):\n",
    "        os.mkdir('./output_weights/target/{}'.format(exp_name))\n",
    "\n",
    "    if simconfig == 0:\n",
    "        sumoCmd = [\"/usr/bin/sumo/bin/sumo\", \"-c\", \n",
    "                   \"../sumo_simulation/sim_config/km2_centro/scenario/osm.sumocfg\"]\n",
    "    else:\n",
    "        sumoCmd = [\"/usr/bin/sumo/bin/sumo\", \"-c\", \n",
    "                   \"../sumo_simulation/sim_config/km2_centro/scenario_{}/osm.sumocfg\".format(simconfig)]\n",
    "\n",
    "    action_dict = cPickle.load(open('../sumo_simulation/input/action_to_zone_km2_centro.pkl', 'rb'))\n",
    "    \n",
    "    total_steps_1sim = 0\n",
    "    \n",
    "    traci.start(sumoCmd)\n",
    "    while traci.simulation.getMinExpectedNumber() > 0:\n",
    "        traci.simulationStep()\n",
    "        total_steps_1sim += 1\n",
    "    traci.close(False)\n",
    "\n",
    "    with open('output/rewards_gamma001_experiment_{}'.format(date), 'a') as reward_file:\n",
    "        for i_episode in range(num_episodes):\n",
    "            print(\"Episode {}\".format(i_episode))\n",
    "\n",
    "            state = np.zeros(6)\n",
    "            reward = 0\n",
    "            done = 0\n",
    "\n",
    "            #Start simulation\n",
    "            traci.start(sumoCmd)\n",
    "            id_list = traci.edge.getIDList()\n",
    "            lane_id_list = traci.lane.getIDList()\n",
    "\n",
    "            #Run simulation steps\n",
    "            traci_ep = 0\n",
    "            while traci.simulation.getMinExpectedNumber() > 0:\n",
    "\n",
    "                if traci_ep >= 2 * total_steps_1sim:\n",
    "                    reward -= 800000\n",
    "                    break\n",
    "\n",
    "                if traci_ep % 500 == 0 and traci_ep != 0:\n",
    "\n",
    "                    #Start agent interaction\n",
    "                    action = select_action(torch.tensor(state, device=device, dtype=torch.float))\n",
    "\n",
    "                    #Apply regulation and run steps\n",
    "                    reg_action = action > 0\n",
    "                    #lane_indices = (reg_action == 1).nonzero().view(-1)\n",
    "\n",
    "                    for index, lane_id in enumerate(reg_action.view(-1)):\n",
    "                    #for lane_id in lane_indices:\n",
    "                        if lane_id.item() == 1:\n",
    "                            if action_dict[index] is not None:\n",
    "                                traci.lane.setDisallowed(action_dict[index], ['truck'])\n",
    "                            else:\n",
    "                                reward -= 10000\n",
    "                        else:\n",
    "                            if action_dict[index] is not None:\n",
    "                                traci.lane.setAllowed(action_dict[index], ['truck'])\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "                    #Get simulation values\n",
    "                    co2 = [traci.lane.getCO2Emission(edge_id) for edge_id in lane_id_list]\n",
    "                    co = [traci.lane.getCOEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    nox = [traci.lane.getNOxEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    pmx = [traci.lane.getPMxEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    noise = [traci.lane.getNoiseEmission(edge_id) for edge_id in lane_id_list]\n",
    "                    fuel = [traci.lane.getFuelConsumption(edge_id) for edge_id in lane_id_list]\n",
    "\n",
    "                    sim_results = np.array([co2, co, pmx, nox, noise, fuel])\n",
    "\n",
    "                    next_state = np.transpose(sim_results).mean(axis=0)\n",
    "\n",
    "                    vehicle_id_list = traci.vehicle.getIDList()\n",
    "                    vehicle_types = [traci.vehicle.getTypeID(v_id) for v_id in vehicle_id_list]\n",
    "                    vehicle_co2 = [traci.vehicle.getCO2Emission(v_id) for i, v_id in enumerate(vehicle_id_list)\n",
    "                                  if 'truck' in vehicle_types[i]]\n",
    "\n",
    "                    try:\n",
    "                        reward += (sum(vehicle_co2)/len(vehicle_co2)) * -1\n",
    "                    except:\n",
    "                        reward += 0\n",
    "\n",
    "                    #Convert to torch tensors\n",
    "                    reward = torch.tensor([reward], device=device, dtype=torch.float)\n",
    "                    state = torch.tensor(state, device=device, dtype=torch.float)\n",
    "                    next_state = torch.tensor(next_state, device=device, dtype=torch.float)\n",
    "\n",
    "                    print('Reward at timestep {t}: {r}'.format(t=traci_ep/500,r=reward.item()))\n",
    "                    reward_file.write(','.join([str(i_episode), str(traci_ep/500), str(reward.item())])+r'\\n')\n",
    "\n",
    "\n",
    "                    action = torch.tensor(action, device=device, dtype=torch.long).view(-1)\n",
    "                    action = (action == 1)#.nonzero().view(-1)\n",
    "\n",
    "                    if state.shape[0] == 0:\n",
    "                        memory.push(torch.zeros((1,12), device=device), action, next_state, reward)\n",
    "                    else:\n",
    "                        memory.push(state, action, next_state, reward)\n",
    "\n",
    "                    state += next_state\n",
    "                    # Perform one step of the optimization (on the target network)\n",
    "                    optimize_model()\n",
    "\n",
    "                traci.simulationStep()\n",
    "                traci_ep += 1\n",
    "\n",
    "            traci.close(False)\n",
    "\n",
    "             # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                 target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            torch.save(target_net.state_dict(), './output_weights/target/{}/target_net_weights_experiment_ep_{}.pt'.format(exp_name, i_episode))\n",
    "            torch.save(policy_net.state_dict(), './output_weights/policy/{}/policy_net_weights_experiment_ep_{}.pt'.format(exp_name, i_episode))\n",
    "        print('Complete')\n",
    "        torch.save(target_net.state_dict(), './output_weights/target/{}/target_net_weights_experiment_ep_{}.pt'.format(exp_name, i_episode))\n",
    "        torch.save(policy_net.state_dict(), './output_weights/policy/{}/policy_net_weights_experiment_ep_{}.pt'.format(exp_name, i_episode))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "1727ce8a9162459e86a1f96932dfddbb",
   "lastKernelId": "3daec33a-f9f2-4cfd-90df-893fe9e3ca6e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
