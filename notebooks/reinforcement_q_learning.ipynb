{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import city_simulation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from gc import collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_fn = lambda x: 1 if x > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_f = np.vectorize(step_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('city_simulation-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.mlp1 = nn.Linear(12,32)\n",
    "        self.mlp2 = nn.Linear(32,64)\n",
    "        self.mlp3 = nn.Linear(64,128)\n",
    "        self.mlp4 = nn.Linear(128,256)\n",
    "        self.mlp5 = nn.Linear(256,256)\n",
    "        self.head = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "        x = F.relu(self.mlp4(x))\n",
    "        x = F.relu(self.mlp5(x))\n",
    "    \n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 5981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (mlp1): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (mlp2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (mlp3): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (mlp4): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (mlp5): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (head): Linear(in_features=256, out_features=5981, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    policy_net = DQN(n_actions).to(device)\n",
    "    target_net = DQN(n_actions).to(device)\n",
    "    policy_net = nn.DataParallel(policy_net)\n",
    "    target_net = nn.DataParallel(target_net)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "else:\n",
    "    print('Using 1 GPU')\n",
    "    policy_net = DQN(n_actions).to(device)\n",
    "    target_net = DQN(n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "policy_net.to(device)\n",
    "target_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state.view([-1,12]))\n",
    "    else:\n",
    "        return torch.randn(n_actions, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)#.view(-1,1,12)\n",
    "    #action_batch = torch.cat(batch.action).view(-1,n_actions)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #print(state_batch.shape, action_batch.shape)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch)#.gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    nsv = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    #print(state_action_values.shape, nsv.shape, next_state_values.shape, non_final_mask.shape)\n",
    "    \n",
    "    next_state_values[non_final_mask] = nsv\n",
    "    \n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.0955581665039\n",
      "Reward at timestep 1: 82.19066619873047\n",
      "Reward at timestep 2: 82.15876770019531\n",
      "Reward at timestep 3: 82.21080017089844\n",
      "Reward at timestep 4: 82.19380187988281\n",
      "Reward at timestep 5: 82.17118072509766\n",
      "Reward at timestep 6: 82.26754760742188\n",
      "Reward at timestep 7: 82.49482727050781\n",
      "Reward at timestep 8: 82.49663543701172\n",
      "Episode 1\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.2548599243164\n",
      "Reward at timestep 1: 82.2188491821289\n",
      "Reward at timestep 2: 82.0751724243164\n",
      "Reward at timestep 3: 81.9838638305664\n",
      "Reward at timestep 4: 81.97872924804688\n",
      "Reward at timestep 5: 82.0020751953125\n",
      "Reward at timestep 6: 81.94097137451172\n",
      "Reward at timestep 7: 82.22924041748047\n",
      "Reward at timestep 8: 82.35343170166016\n",
      "Reward at timestep 9: 82.45510864257812\n",
      "Reward at timestep 10: 82.46837615966797\n",
      "Reward at timestep 11: 82.48808288574219\n",
      "Reward at timestep 12: 82.49663543701172\n",
      "Episode 2\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.01276397705078\n",
      "Reward at timestep 1: 81.92506408691406\n",
      "Reward at timestep 2: 81.79505157470703\n",
      "Reward at timestep 3: 81.74956512451172\n",
      "Reward at timestep 4: 81.61409759521484\n",
      "Reward at timestep 5: 81.46942901611328\n",
      "Reward at timestep 6: 81.32791137695312\n",
      "Reward at timestep 7: 81.59590911865234\n",
      "Reward at timestep 8: 81.78182220458984\n",
      "Reward at timestep 9: 81.86343383789062\n",
      "Reward at timestep 10: 82.09439849853516\n",
      "Reward at timestep 11: 82.16632080078125\n",
      "Reward at timestep 12: 82.2748794555664\n",
      "Reward at timestep 13: 82.2983627319336\n",
      "Reward at timestep 14: 82.37078094482422\n",
      "Reward at timestep 15: 82.4098892211914\n",
      "Reward at timestep 16: 82.4404067993164\n",
      "Reward at timestep 17: 82.43812561035156\n",
      "Reward at timestep 18: 82.46292877197266\n",
      "Reward at timestep 19: 82.47615814208984\n",
      "Reward at timestep 20: 82.48091888427734\n",
      "Reward at timestep 21: 82.48937225341797\n",
      "Reward at timestep 22: 82.4936752319336\n",
      "Reward at timestep 23: 82.49663543701172\n",
      "Episode 3\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.04007720947266\n",
      "Reward at timestep 1: 81.91197967529297\n",
      "Reward at timestep 2: 81.82919311523438\n",
      "Reward at timestep 3: 81.63650512695312\n",
      "Reward at timestep 4: 81.5243911743164\n",
      "Reward at timestep 5: 81.3331298828125\n",
      "Reward at timestep 6: 81.2122573852539\n",
      "Reward at timestep 7: 81.47574615478516\n",
      "Reward at timestep 8: 81.69882202148438\n",
      "Reward at timestep 9: 81.81526184082031\n",
      "Reward at timestep 10: 81.91921997070312\n",
      "Reward at timestep 11: 82.03581237792969\n",
      "Reward at timestep 12: 82.17341613769531\n",
      "Reward at timestep 13: 82.22020721435547\n",
      "Reward at timestep 14: 82.3171157836914\n",
      "Reward at timestep 15: 82.38581848144531\n",
      "Reward at timestep 16: 82.42205047607422\n",
      "Reward at timestep 17: 82.43098449707031\n",
      "Reward at timestep 18: 82.45442199707031\n",
      "Reward at timestep 19: 82.47156524658203\n",
      "Reward at timestep 20: 82.4793701171875\n",
      "Reward at timestep 21: 82.48283386230469\n",
      "Reward at timestep 22: 82.47866821289062\n",
      "Reward at timestep 23: 82.49058532714844\n",
      "Reward at timestep 24: 82.49662780761719\n",
      "Reward at timestep 25: 82.49663543701172\n",
      "Episode 4\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.22305297851562\n",
      "Reward at timestep 1: 82.28366088867188\n",
      "Reward at timestep 2: 82.2439956665039\n",
      "Reward at timestep 3: 82.20470428466797\n",
      "Reward at timestep 4: 82.23249816894531\n",
      "Reward at timestep 5: 82.12523651123047\n",
      "Reward at timestep 6: 82.07765197753906\n",
      "Reward at timestep 7: 82.3553695678711\n",
      "Reward at timestep 8: 82.40533447265625\n",
      "Reward at timestep 9: 82.41461944580078\n",
      "Reward at timestep 10: 82.45341491699219\n",
      "Reward at timestep 11: 82.4768295288086\n",
      "Reward at timestep 12: 82.4864501953125\n",
      "Reward at timestep 13: 82.49663543701172\n",
      "Episode 5\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.31875610351562\n",
      "Reward at timestep 1: 82.27764129638672\n",
      "Reward at timestep 2: 82.33251953125\n",
      "Reward at timestep 3: 82.27586364746094\n",
      "Reward at timestep 4: 82.26139068603516\n",
      "Reward at timestep 5: 82.30029296875\n",
      "Reward at timestep 6: 82.25852966308594\n",
      "Reward at timestep 7: 82.49663543701172\n",
      "Episode 6\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.31875610351562\n",
      "Reward at timestep 1: 82.27764129638672\n",
      "Reward at timestep 2: 82.33251953125\n",
      "Reward at timestep 3: 82.27586364746094\n",
      "Reward at timestep 4: 82.26139068603516\n",
      "Reward at timestep 5: 82.30029296875\n",
      "Reward at timestep 6: 82.25852966308594\n",
      "Reward at timestep 7: 82.49663543701172\n",
      "Episode 7\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.26640319824219\n",
      "Reward at timestep 1: 82.30500030517578\n",
      "Reward at timestep 2: 82.31733703613281\n",
      "Reward at timestep 3: 82.3153076171875\n",
      "Reward at timestep 4: 82.30496978759766\n",
      "Reward at timestep 5: 82.3052749633789\n",
      "Reward at timestep 6: 82.33628845214844\n",
      "Reward at timestep 7: 82.46551513671875\n",
      "Reward at timestep 8: 82.48619842529297\n",
      "Reward at timestep 9: 82.48737335205078\n",
      "Reward at timestep 10: 82.49663543701172\n",
      "Episode 8\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.31875610351562\n",
      "Reward at timestep 1: 82.27764129638672\n",
      "Reward at timestep 2: 82.33251953125\n",
      "Reward at timestep 3: 82.27586364746094\n",
      "Reward at timestep 4: 82.26139068603516\n",
      "Reward at timestep 5: 82.30029296875\n",
      "Reward at timestep 6: 82.25852966308594\n",
      "Reward at timestep 7: 82.49663543701172\n",
      "Episode 9\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.18759155273438\n",
      "Reward at timestep 1: 82.17900085449219\n",
      "Reward at timestep 2: 82.17581939697266\n",
      "Reward at timestep 3: 82.1496353149414\n",
      "Reward at timestep 4: 82.16443634033203\n",
      "Reward at timestep 5: 82.09465026855469\n",
      "Reward at timestep 6: 82.05084228515625\n",
      "Reward at timestep 7: 82.30802154541016\n",
      "Reward at timestep 8: 82.32808685302734\n",
      "Reward at timestep 9: 82.35445404052734\n",
      "Reward at timestep 10: 82.36160278320312\n",
      "Reward at timestep 11: 82.38472747802734\n",
      "Reward at timestep 12: 82.39010620117188\n",
      "Reward at timestep 13: 82.39971923828125\n",
      "Reward at timestep 14: 82.4383544921875\n",
      "Reward at timestep 15: 82.42224884033203\n",
      "Reward at timestep 16: 82.42375946044922\n",
      "Reward at timestep 17: 82.4420166015625\n",
      "Reward at timestep 18: 82.43395233154297\n",
      "Reward at timestep 19: 82.44264221191406\n",
      "Reward at timestep 20: 82.4441909790039\n",
      "Reward at timestep 21: 82.45094299316406\n",
      "Reward at timestep 22: 82.45203399658203\n",
      "Reward at timestep 23: 82.45720672607422\n",
      "Reward at timestep 24: 82.46136474609375\n",
      "Reward at timestep 25: 82.46602630615234\n",
      "Reward at timestep 26: 82.46986389160156\n",
      "Reward at timestep 27: 82.474365234375\n",
      "Reward at timestep 28: 82.47810363769531\n",
      "Reward at timestep 29: 82.4942398071289\n",
      "Reward at timestep 30: 82.49663543701172\n",
      "Episode 10\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 81.97622680664062\n",
      "Reward at timestep 1: 82.00106811523438\n",
      "Reward at timestep 2: 81.94054412841797\n",
      "Reward at timestep 3: 81.8276138305664\n",
      "Reward at timestep 4: 81.70829772949219\n",
      "Reward at timestep 5: 81.59701538085938\n",
      "Reward at timestep 6: 81.50872039794922\n",
      "Reward at timestep 7: 81.72200012207031\n",
      "Reward at timestep 8: 81.89634704589844\n",
      "Reward at timestep 9: 82.01921844482422\n",
      "Reward at timestep 10: 82.12337493896484\n",
      "Reward at timestep 11: 82.20731353759766\n",
      "Reward at timestep 12: 82.29776763916016\n",
      "Reward at timestep 13: 82.34712219238281\n",
      "Reward at timestep 14: 82.3954849243164\n",
      "Reward at timestep 15: 82.43052673339844\n",
      "Reward at timestep 16: 82.43739318847656\n",
      "Reward at timestep 17: 82.45899963378906\n",
      "Reward at timestep 18: 82.46757507324219\n",
      "Reward at timestep 19: 82.48553466796875\n",
      "Reward at timestep 20: 82.49031066894531\n",
      "Reward at timestep 21: 82.49075317382812\n",
      "Reward at timestep 22: 82.49454498291016\n",
      "Reward at timestep 23: 82.49663543701172\n",
      "Episode 11\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.28653717041016\n",
      "Reward at timestep 1: 82.25434112548828\n",
      "Reward at timestep 2: 82.2202377319336\n",
      "Reward at timestep 3: 82.0897216796875\n",
      "Reward at timestep 4: 82.02261352539062\n",
      "Reward at timestep 5: 81.98767852783203\n",
      "Reward at timestep 6: 81.8899917602539\n",
      "Reward at timestep 7: 81.99933624267578\n",
      "Reward at timestep 8: 82.12274932861328\n",
      "Reward at timestep 9: 82.14854431152344\n",
      "Reward at timestep 10: 82.2164306640625\n",
      "Reward at timestep 11: 82.24463653564453\n",
      "Reward at timestep 12: 82.31658172607422\n",
      "Reward at timestep 13: 82.4637680053711\n",
      "Reward at timestep 14: 82.49663543701172\n",
      "Episode 12\n",
      " Retrying in 1 seconds\n",
      "Reward at timestep 0: 82.22305297851562\n",
      "Reward at timestep 1: 82.28366088867188\n",
      "Reward at timestep 2: 82.2439956665039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward at timestep 3: 82.20470428466797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-068d27c7e0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tesis_pregrado/sumo_simulation/city_simulation/city_simulation/envs/city_simulation_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignAllowedVehicles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mreward_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunSimulationSteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;31m#self.iteration_counter += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tesis_pregrado/sumo_simulation/city_simulation/city_simulation/envs/city_simulation_env.py\u001b[0m in \u001b[0;36mrunSimulationSteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m#subscriptions_list.append(pd.DataFrame.from_dict(traci.lane.getAllSubscriptionResults()).values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0msim_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlane\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAllSubscriptionResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#np.sum(subscriptions_list, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_ep = 0\n",
    "\n",
    "with open('output/rewards_experiment_{}'.format(str(datetime.datetime.today())[:-7]), 'a') as reward_file:\n",
    "\n",
    "    num_episodes = 400\n",
    "    for i_episode in range(num_episodes):\n",
    "        print(\"Episode {}\".format(i_episode))\n",
    "        # Initialize the environment and state\n",
    "        env.reset()  \n",
    "        for t in count():\n",
    "            current_ep += 1\n",
    "            state = env.render()\n",
    "            action = select_action(torch.tensor(state, device=device, dtype=torch.float))\n",
    "            next_state, reward, done, _ = env.step(action > 0.5)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            state = torch.tensor(state, device=device, dtype=torch.float)\n",
    "            next_state = torch.tensor(next_state, device=device, dtype=torch.float)\n",
    "\n",
    "            print('Reward at timestep {t}: {r}'.format(t=t,r=reward.item()))\n",
    "            \n",
    "            reward_file.write(','.join([str(i_episode), str(current_ep), str(reward.item())])+r'\\n')\n",
    "\n",
    "            if state.view(-1).shape == 12:\n",
    "                state = state.view(-1)\n",
    "                next_state = next_state.view(-1)\n",
    "            else:\n",
    "                state = state.view(-1,12)\n",
    "                next_state = next_state.view(-1,12)\n",
    "\n",
    "            action = torch.tensor(action, device=device, dtype=torch.long).view(-1)\n",
    "            action = (action == 1).nonzero().view(-1)\n",
    "\n",
    "            if state.shape[0] == 0:\n",
    "                memory.push(torch.zeros((1,12), device=device), action, next_state, reward)\n",
    "            else:\n",
    "                memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model()\n",
    "            collect()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                collect()\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            collect()\n",
    "            \n",
    "        torch.save(target_net.state_dict(), './output_weights/target/target_net_weights_{}_ep_{}.pt'.format(str(datetime.datetime.today())[:-7], i_episode))\n",
    "        torch.save(policy_net.state_dict(), './output_weights/policy/policy_net_weights_{}_ep_{}.pt'.format(str(datetime.datetime.today())[:-7], i_episode))\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "torch.save(target_net.state_dict(), './output_weights/target_net.pt')\n",
    "torch.save(policy_net.state_dict(), './output_weights/policy_net.pt')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "71062617fecf4dff8a83714bfa493e54",
   "lastKernelId": "c1f82b09-406b-40eb-9f8e-74f5b5a51fe5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
